\documentclass[acmsmall]{acmart}
% \documentclass[letterpaper]{llncs}
% \usepackage[letterpaper, margin=1.5in]{geometry}

\usepackage{mathpartir}
\usepackage{hyperref}
\usepackage{mathtools}
\usepackage{amsmath}
\usepackage{nccmath}
\usepackage{stmaryrd}
\usepackage{listings}


\usepackage{graphicx}
\graphicspath{ {./images/} }

\usepackage{url}

\makeatletter % allow us to mention @-commands
\def\arcr{\@arraycr}
\makeatother

% \lstset{
%     % identifierstyle=\color{violet},
%     % textcolor=blue,
%     % keywordstyle=\color{blue},
%     keywordstyle=\text,
%     basicstyle=\ttfamily,
%     mathescape=true,
%     showspaces=false,
%     morekeywords={let, fix, in}
% }
% \usepackage[utf8]{inputenc}
% % \usepackage[T1]{fontenc}


\title{Relational Type Inference}
\author{Thomas Logan}
% \date{October 2022}

\begin{document}

\maketitle


\section{Introduction}

\paragraph{Context.} 
Automatically catching errors in programs is a hard enough problem
that many languages require users to provide simple specifications to limit that space of correctness.
Languages, such as Java and ML, are \textit{intrinsically typed}, 
requiring nearly all terms to be associated with some type specified by the user. 
The clever design of ML allows annotations to be fairly sparse by 
having types specified at constructor definitions and relying on type inference elsewhere.
For various reasons that aren't completely clear, intrinsically typed languages have lost favor,
and untyped or \textit{extrinsically typed} languages, 
such as Javascript/Typescript and Python, have surged in popularity. 

Despite the ever increasing use of untyped languages in production systems, 
the need to automatically verify precise and expressive properties of systems has never been greater.
To this end, researchers have extended the simple types found in Java and ML into 
\textit{refinement types}, \textit{predicate subtyping}, and \textit{dependent types}. 
Refinement types offer greater precision than simple types, but still rely on intrinsic type specifications.
Dependent types can express detailed relations, but may require users to provide proofs along with detailed annotations.
Predicate subtyping offers some of the expressivity of dependent types, but with the automatic subtyping of refinement types.
All of these techniques are based on intrinsic typing and therefore require users to provide additional annotations
beyond the runtime behavior of their programs.

\paragraph{Gap.} 
Unfortunately, since typing specifications are optional in extrinsically typed languages,
there are many untyped programs that cannot benefit from the intrinsically typed techniques,
not to mention techniques that require users to provide proofs.
For instance, the liquid type system \cite{} can verify and infer some relational properties, 
but it requires users to specify ML-style base types and a set of logical qualifiers to draw from.

While these intrinsic-based techniques could be useful if untyped programs were annotated,
the effort to manually annotate untyped programs is often enormous and error-prone, 
especially in cases where a programmer is using code written by third parties.
Thus, it is has become increasingly important to develop techniques 
that can automatically infer expressive types from untyped programs.

\paragraph{Innovation.} 
To overcome these limitations, we introduce a new system 
that automatically infers expressive properties from untyped functional programs. 
We introduce a new type language \textit{relational types} to represent these properties.

\section{Overview}

\subsection{Language of types}

\paragraph{Parametric types}
Universal types. Existential type. System F-style. Parameterization of types indexed by types (i.e. second order).

\paragraph{Combination types.}
One of the advantages of untyped programs is that they may be written in a flexible manner.
Subtyping is necessary safely reflect the flexibility of compositions in programs, without too many false failures.
Another main advantage of untyped programs is that users don't have to provide type specifications.
Thus, a general way of constructing types from compositions encountered in the the program is necessary.
Some compositions indicate that a type should strengthen, and some compositions indicate that a type should weaken.
To this end, the type language uses intersection and union combinators, 
whose semantics are degenerate versions of those in set-theory.

For instance, when inferring the type of a function, 
the system's goal is to infer the weakest valid parameter type and the strongest valid return type for a function definition.
It strengthens the parameter type with intersection and weakens the return type with union according to the function body,
to arrive at a valid type for the function.  

By contrast, the liquid type language relies on the less flexible tagged unions of ML datatypes, 
which is sufficient in its setting since those types are specified by the user. 
Likewise, it does not rely on union to weaken to a valid return type. 
Instead, it weakens to the strongest valid return type by dropping conjunctions from 
the return type's qualifiers until a valid return type is found.

\paragraph{Inductive types.} Similar to ML datatypes.

\paragraph{Constraint types.}
In addition to expressing the shapes of terms, the system should be able express relations between terms,
such as "a list has the length of some natural number".
Rather than using a distinct syntax for relational predicates, 
the type language treats relations as just another type thereby reusing machinery already 
available for types, such as existential types, union types, and inductive types.
Since parametric types are second order, constraining relations requires subtyping.
Thus, parametric types are extended with constraints in the form of subtyping.



\subsection{Basic Type Inference}

We begin with some simple examples to illustrate the syntax and semantics of
programs and types.

\paragraph{Path typing.}
Consider a trivial function that completes an English phrase:
\begin{mathpar}
\\
  \inferrule {} {
    trivial = 
    \begin{array}[t]{@{} l}
        \texttt{lam \#hello() => \#world()}
        \arcr
        \texttt{lam \#good() => \#morning()}
    \end{array}
  }
\\
\end{mathpar}

This program is defined by paths over hardcoded tags.
The system infers the type to be an intersection of implication types:
\begin{mathpar}
\\
  \inferrule {} {
    \Delta \cdot \Gamma
    \vdash 
    trivial : 
        \texttt{(?hello unit -> ?world unit) \& (?good unit -> ?morning unit)}
  }
\\
\end{mathpar}

\paragraph{Path selection.}

Suppose the program $trivial$ is applied to a literal value $\#\text{hello}()$. 
The system can discard the irrelevant clauses and infer the singleton type.
\begin{mathpar}
\\
    \inferrule {} {
        \Delta \cdot \Gamma
        \vdash 
        trivial\ \texttt{\#hello()} :\ \texttt{?world unit} 
    }
\\
\end{mathpar}


\paragraph{Relational typing.} Consider a function that takes a natural number and returns a list of that length. 
\begin{mathpar}
\\
  \inferrule {} {
    repeat = 
    \begin{array}[t]{@{} l}
        \texttt{lam x => fix (lam self =>}
        \arcr
        \hspace{4mm}\texttt{lam \#zero() => \#nil()}
        \arcr
        \hspace{4mm}\texttt{lam \#succ n => \#cons (x, self n))}
    \end{array}
  }
\\
\end{mathpar}
Without specifying any requirements besides the function definition, the system 
can infer the property that the resulting list has the length of the input number. 
\begin{mathpar}
\\
  \inferrule {
    nat = \texttt{induct N . ?zero unit | ?succ N} 
    \\\\
    nat\_list =  
    \begin{array}[t]{@{} l}
        \texttt{induct NL .} 
        \arcr
        \hspace{4mm} \texttt{?zero unit * ?nil unit |}
        \arcr
        \hspace{4mm} \texttt{\{?succ N * ?cons L with N * L <: NL\}}
    \end{array}
  } {
    \Delta \cdot \Gamma
    \vdash 
    repeat : \texttt{forall N <:}\ nat \texttt{ . N -> \{L with N * L <:}\ nat\_list \texttt{\}} 
  }
\\
\end{mathpar}

\paragraph{Relational selection.} Suppose the program $repeat$ is applied to the hardcoded number two, 
represented as \texttt{ \#succ \#succ \#zero ()} . The system can infer the singleton type representing a list,
much like how Prolog evaluates logic programs.
\begin{mathpar}
\\
  \inferrule {} {
    \Delta \cdot \Gamma
    \vdash 
    repeat\ \texttt{()  (\#succ \#succ \#zero ())}:  \texttt{\#cons (unit * \#cons (unit * \#nil unit))}
  }
\\
\end{mathpar}

Path selection and relational selection demonstrate that the declarative type language is expressive enough
to perform evaluation. However, this simply reproduces the effect of the dynamic semantics, 
albeit in a declarative style. For types to be useful in practice, they need to offer ways to
express properties with incomplete information.


\subsection{Practical Type Inference}
Next we demonstrate how types representing abstract properties compose to produce useful properties,
which are not reproducible by dynamic semantics.

\paragraph{Transposition.} Consider a function that expects a list, applied to a list that's related to a nat. 
\begin{mathpar}
\\
  \inferrule {} {
    \begin{array}[t]{@{} l}
        \texttt{lam x n => }
        \arcr
        \hspace{4mm}\texttt{let repeat = } repeat \texttt{ in}
        \arcr
        \hspace{4mm}\texttt{let fromList = fix (lam self =>}
        \arcr
        \hspace{4mm}\hspace{4mm}\texttt{lam \#nil() => }\hdots
        \arcr
        \hspace{4mm}\hspace{4mm}\texttt{lam \#cons (x, xs) => }\hdots \texttt{) in}
        \arcr
        \hspace{4mm}\texttt{fromList (repeat x n)}
    \end{array}
  }
\\
\end{mathpar}

The parameter type of \texttt{(fromList)} is: 
\begin{mathpar}
\\
  \inferrule {} {
    list\ \alpha = \texttt{induct L . ?nil unit | ?cons (} \alpha \texttt{ * L)} 
  }
\\
\end{mathpar}
However, the argument type of the term \texttt{(repeat x n)} is:
\begin{mathpar}
\\
  \inferrule {} {
    \texttt{\{L with X * L <:}\ nat\_list \texttt{\}}
  }
\\
\end{mathpar}

The types need to be fairly complex due to the abstract information available.
Despite these complexities, the the system is able to ensure that these abstract inferred types
can safely be composed, by transposing $nat\_list$ into the weaker pair $nat$ \texttt{*} $list \alpha$,
and then unifying the argument's projected list type with the parameter's list type.





\paragraph{Inductive subtyping.} An even number subtypes a natural number.
\paragraph{Relational subtyping.} An evenList subtypes a natList.
But the argument type is a Nat related to a List.
To check safety, the NatList relation must be transposed into a pair of Nat and List.
\paragraph{Refining parameter type.}
\paragraph{Path sensitivity.}
\paragraph{List bounds.}
\paragraph{Generalization.} This is motivated by applying the same function to multiple arguments of varying types.
\paragraph{Expanding return type.}  
This is motivated by object-oriented patterns in untyped programs.
The expansion flag may not actually be needed, as distinct types can be handled via existential inside of inductive type,
which results in fresh variables at the time of unification.

\section{Related work}

\paragraph{Hindley-Milner type inference}
Exemplified by ML.

\paragraph{Logic programming.}
Exemplified by Prolog. 

Similar: both have backchaining. 

Different: RLT is fully declarative, lacks negations, but has implication. 

Different: RLT allows comparing inductive relations via subtyping. 

\paragraph{Semantic subtyping.} 
Exemplified by XDuce and CDuce. complete subtyping.

Similar: set-like combinators: union and intersection.

Different: RLT uses rigid syntactic rules; incomplete subtyping.

% set theoretic notes: https://pnwamk.github.io/sst-tutorial/#%28part._sec~3asemantic-subtyping%29
% semantic subtyping: https://www.irif.fr/~gc/papers/semantic_subtyping.pdf
The terminology "semantic subtyping" vs "syntactic subtyping" are confusing terms. 
"semantics subtyping" means the semantics of types is determined indirectly by the semantics of another structure.
"syntactic subtyping" means the semantics of types is determined directly by the type structure

In my opinion, better terms would be "model-based" vs "proof-based".
A model-based system can be more complete but requires proving absence of inhabitation of the types.
That is, the semantics of subtyping would depend on the semantics of inhabitation of types.
This is related to SAT solving, proof synthesis, model checking, and SMT.
If a type is model-based, not proof-based, then a proof must be found.
Since the "proof" is not part of the model-based subtyping statement, 
there is an infinite space to search to prove that the subtyping statement holds.
This leads to a more complete system, but a more difficult system to decide.

\paragraph{Extrinsic typing.}
Exemplified by Typescript, which is unsound. Maybe not as lenient?  
The static behavior of a program is not necessarily specified/prescribed; 
it may be over-approximated from the program composition. 
Intrinsic vs extrinsic is orthogonal to static vs dynamic, although static and dynamic are often used to mean the former.
All modern languages use a combination of static and dynamic type checking.
The term "dynamically typed" some times refers to a language that doesn't prescribe static meaning,
even if it uses both static and dynamic type checking. The term "extrinsic typing" is less ambiguous.

\paragraph{Refinement Types.}
Exemplified by Refinement ML. Base types with intersections and subtyping.

\paragraph{Predicate Subtyping.}
Exemplified by Liquid Types. An extension of refinement types.

Similar: both use type inference to infer expressive relational properties. 

Different: RLT starts with an invalid post-condition, then weakens return type to a valid post-condition from outside in by expanding unions.

Different: RLT starts with an invalid pre-condition, then strengthens parameter type to a valid pre-condition from inside out by adding intersections.

Different: Liquid types starts with an invalid post-condition, then uses iterative weakening by dropping conjunctions until a valid post-condition is reached.


\paragraph{Abstraction Refinement.} 
Similar: type unification over subtyping resembles abstraction refinement  
where solving for variables and failing on different sides of the subtyping relation corresponds to
solving with the abstractor vs solving with the refiner.

\paragraph{Craig interpolation.} 
Similar: extracting an inductive type with unions and intersections 
from a recursive program without needing to specify a predicate universe might be similar to
craig interpolation.

\paragraph{PDR.}
exemplified by IC3. 

Similar: RLT infers abstract type for return type, 
then safely constrains the variables in previous step (fix's antecedent) 
to subtype the least fixed point.
This lazily propagates the type for the last step to the previous steps.
This is safe as antecedent is stronger than consequent at any step.
Seems similar to the notion in PDR of propagating negation of loss points to previous steps. 

Different: RLT isn't cartesian

\end{document}


